{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqq2PvZsNrJhhInq2WtHyA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bintangnabiil/Deep_Learning/blob/main/Copy_of_Deep_Learning_Bidirectional_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A. IMDb Dataset"
      ],
      "metadata": {
        "id": "ueEqw66X7Bp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Load IMDb Dataset"
      ],
      "metadata": {
        "id": "aB2AsDOr7JW-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmi-RYF950T8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "(imdb_train_data, imdb_test_data), imdb_info = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split=['train', 'test'],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) Preprocessing Data"
      ],
      "metadata": {
        "id": "Hcw2rm1C7gyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "imdb_train_sentences = []\n",
        "imdb_train_labels = []\n",
        "for s, l in tfds.as_numpy(imdb_train_data):\n",
        "    imdb_train_sentences.append(s.decode('utf-8'))\n",
        "    imdb_train_labels.append(l)\n",
        "\n",
        "imdb_test_sentences = []\n",
        "imdb_test_labels = []\n",
        "for s, l in tfds.as_numpy(imdb_test_data):\n",
        "    imdb_test_sentences.append(s.decode('utf-8'))\n",
        "    imdb_test_labels.append(l)\n",
        "\n",
        "imdb_tokenizer.fit_on_texts(imdb_train_sentences)\n",
        "imdb_train_sequences = imdb_tokenizer.texts_to_sequences(imdb_train_sentences)\n",
        "imdb_test_sequences = imdb_tokenizer.texts_to_sequences(imdb_test_sentences)\n",
        "\n",
        "imdb_max_length = 200\n",
        "imdb_train_padded = pad_sequences(imdb_train_sequences, maxlen=imdb_max_length, truncating='post')\n",
        "imdb_test_padded = pad_sequences(imdb_test_sequences, maxlen=imdb_max_length, truncating='post')\n",
        "\n",
        "imdb_train_labels = tf.convert_to_tensor(imdb_train_labels)\n",
        "imdb_test_labels = tf.convert_to_tensor(imdb_test_labels)"
      ],
      "metadata": {
        "id": "o0yq0czg6vwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Build BiRNN Model"
      ],
      "metadata": {
        "id": "z8VGXsRR7u-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=64, input_length=imdb_max_length),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "imdb_model.compile(loss='binary_crossentropy',\n",
        "                   optimizer='adam',\n",
        "                   metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "e3gv0TBo72p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4) Train Model"
      ],
      "metadata": {
        "id": "qfjOG85A74vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_history = imdb_model.fit(imdb_train_padded, imdb_train_labels,\n",
        "                              validation_data=(imdb_test_padded, imdb_test_labels),\n",
        "                              epochs=5,\n",
        "                              batch_size=128)"
      ],
      "metadata": {
        "id": "mnkt_IBV76ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5) Evaluasi Model"
      ],
      "metadata": {
        "id": "0Sc6dgA68Mtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_loss, imdb_accuracy = imdb_model.evaluate(imdb_test_padded, imdb_test_labels)\n",
        "print(f\"IMDb Test Accuracy: {imdb_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "QQiFfLZD8O7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6) Matriks Evaluasi"
      ],
      "metadata": {
        "id": "jcLL4y6i8nRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Prediksi Probabilitas\n",
        "imdb_y_pred_prob = imdb_model.predict(imdb_test_padded)\n",
        "imdb_y_pred = (imdb_y_pred_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(imdb_test_labels, imdb_y_pred, digits=4))\n",
        "\n",
        "# ROC & AUC\n",
        "imdb_auc = roc_auc_score(imdb_test_labels, imdb_y_pred_prob)\n",
        "imdb_fpr, imdb_tpr, _ = roc_curve(imdb_test_labels, imdb_y_pred_prob)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(imdb_fpr, imdb_tpr, label=f'ROC Curve (AUC = {imdb_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - IMDb')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Visualisasi Akurasi dan Loss selama training\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(imdb_history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(imdb_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy - IMDb')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(imdb_history.history['loss'], label='Train Loss')\n",
        "plt.plot(imdb_history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss - IMDb')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TcYckrW-8u4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7) Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "irRTifXZ8-_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Grid of hyperparameter candidates\n",
        "embedding_dims_list = [32, 64]\n",
        "lstm_units_list = [32, 64]\n",
        "batch_sizes = [64, 128]\n",
        "learning_rates = [1e-3, 5e-4]\n",
        "\n",
        "best_val_acc = 0\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "for emb_dim in embedding_dims_list:\n",
        "    for lstm_units in lstm_units_list:\n",
        "        for batch_size in batch_sizes:\n",
        "            for lr in learning_rates:\n",
        "                print(f\"\\nðŸ”§ Training with emb_dim={emb_dim}, lstm_units={lstm_units}, batch_size={batch_size}, lr={lr}\")\n",
        "\n",
        "                model = Sequential([\n",
        "                    Embedding(input_dim=10000, output_dim=emb_dim, input_length=imdb_max_length),\n",
        "                    Bidirectional(LSTM(lstm_units)),\n",
        "                    Dense(64, activation='relu'),\n",
        "                    Dense(1, activation='sigmoid')\n",
        "                ])\n",
        "\n",
        "                model.compile(\n",
        "                    loss='binary_crossentropy',\n",
        "                    optimizer=Adam(learning_rate=lr),\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                history = model.fit(imdb_train_padded, imdb_train_labels,\n",
        "                                    validation_data=(imdb_test_padded, imdb_test_labels),\n",
        "                                    epochs=3,  # untuk efisiensi tuning\n",
        "                                    batch_size=batch_size,\n",
        "                                    verbose=0)\n",
        "\n",
        "                val_acc = history.history['val_accuracy'][-1]\n",
        "                print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    best_model = model\n",
        "                    best_params = {\n",
        "                        'embedding_dim': emb_dim,\n",
        "                        'lstm_units': lstm_units,\n",
        "                        'batch_size': batch_size,\n",
        "                        'learning_rate': lr\n",
        "                    }\n",
        "\n",
        "print(\"\\nâœ… Best Hyperparameters Found:\")\n",
        "print(best_params)\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "BiuCXIcN9CUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#B. ReviewTokoBaju.csv Dataset"
      ],
      "metadata": {
        "id": "rxqn0NNh9Ypw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Load dan Preprocess Data"
      ],
      "metadata": {
        "id": "XM1myVSQC0yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "baju_df = pd.read_csv('/content/ReviewTokoBaju.csv')\n",
        "print(\"Kolom yang tersedia:\", baju_df.columns)\n",
        "baju_df.head()"
      ],
      "metadata": {
        "id": "G6Tn1c1aDOQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "# Load data\n",
        "baju_df = pd.read_csv('/content/ReviewTokoBaju.csv')\n",
        "\n",
        "# Ambil teks dan label\n",
        "baju_texts = baju_df['Review Text'].astype(str).values  # pastikan string\n",
        "baju_labels = baju_df['Recommended IND'].values         # label biner\n",
        "\n",
        "# Split train-test\n",
        "baju_X_train, baju_X_test, baju_y_train, baju_y_test = train_test_split(\n",
        "    baju_texts, baju_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenizer & Padding\n",
        "baju_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "baju_tokenizer.fit_on_texts(baju_X_train)\n",
        "\n",
        "baju_X_train_seq = baju_tokenizer.texts_to_sequences(baju_X_train)\n",
        "baju_X_test_seq = baju_tokenizer.texts_to_sequences(baju_X_test)\n",
        "\n",
        "baju_maxlen = 100\n",
        "baju_X_train_pad = pad_sequences(baju_X_train_seq, maxlen=baju_maxlen, padding='post', truncating='post')\n",
        "baju_X_test_pad = pad_sequences(baju_X_test_seq, maxlen=baju_maxlen, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "o6Ym-pgH9caX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) Build dan Train Model"
      ],
      "metadata": {
        "id": "DlkiMC-xDDoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model BiRNN untuk Review Toko Baju\n",
        "baju_model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=64, input_length=baju_maxlen),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "baju_model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "baju_history = baju_model.fit(\n",
        "    baju_X_train_pad, baju_y_train,\n",
        "    validation_data=(baju_X_test_pad, baju_y_test),\n",
        "    epochs=5,\n",
        "    batch_size=128\n",
        ")\n",
        "\n",
        "# Evaluasi awal\n",
        "baju_loss, baju_acc = baju_model.evaluate(baju_X_test_pad, baju_y_test)\n",
        "print(f\"\\nTest Accuracy (Review Toko Baju): {baju_acc:.4f}\")"
      ],
      "metadata": {
        "id": "ElRGtUYuDIMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Matriks Evaluasi"
      ],
      "metadata": {
        "id": "kC3Q9yzgEHtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prediksi probabilitas dan kelas\n",
        "baju_y_pred_prob = baju_model.predict(baju_X_test_pad)\n",
        "baju_y_pred = (baju_y_pred_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report (Review Toko Baju):\")\n",
        "print(classification_report(baju_y_test, baju_y_pred, digits=4))\n",
        "\n",
        "# ROC & AUC\n",
        "baju_auc = roc_auc_score(baju_y_test, baju_y_pred_prob)\n",
        "baju_fpr, baju_tpr, _ = roc_curve(baju_y_test, baju_y_pred_prob)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(baju_fpr, baju_tpr, label=f'ROC Curve (AUC = {baju_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Review Toko Baju')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot akurasi dan loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(baju_history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(baju_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy - Review Toko Baju')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(baju_history.history['loss'], label='Train Loss')\n",
        "plt.plot(baju_history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss - Review Toko Baju')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eEeJQ4ONEJrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4) Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "oDzmhCKXEb_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Hyperparameter grids\n",
        "baju_embedding_dims = [32, 64]\n",
        "baju_lstm_units = [32, 64]\n",
        "baju_batch_sizes = [64, 128]\n",
        "baju_learning_rates = [1e-3, 5e-4]\n",
        "\n",
        "# Track the best model\n",
        "baju_best_val_acc = 0\n",
        "baju_best_model = None\n",
        "baju_best_params = {}\n",
        "\n",
        "for emb_dim in baju_embedding_dims:\n",
        "    for lstm_unit in baju_lstm_units:\n",
        "        for batch_size in baju_batch_sizes:\n",
        "            for lr in baju_learning_rates:\n",
        "                print(f\"\\nðŸ”§ Training with emb_dim={emb_dim}, lstm_units={lstm_unit}, batch_size={batch_size}, lr={lr}\")\n",
        "\n",
        "                model = Sequential([\n",
        "                    Embedding(input_dim=10000, output_dim=emb_dim, input_length=baju_maxlen),\n",
        "                    Bidirectional(LSTM(lstm_unit)),\n",
        "                    Dense(64, activation='relu'),\n",
        "                    Dense(1, activation='sigmoid')\n",
        "                ])\n",
        "\n",
        "                model.compile(\n",
        "                    loss='binary_crossentropy',\n",
        "                    optimizer=Adam(learning_rate=lr),\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                history = model.fit(\n",
        "                    baju_X_train_pad, baju_y_train,\n",
        "                    validation_data=(baju_X_test_pad, baju_y_test),\n",
        "                    epochs=3,\n",
        "                    batch_size=batch_size,\n",
        "                    verbose=0\n",
        "                )\n",
        "\n",
        "                val_acc = history.history['val_accuracy'][-1]\n",
        "                print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "                if val_acc > baju_best_val_acc:\n",
        "                    baju_best_val_acc = val_acc\n",
        "                    baju_best_model = model\n",
        "                    baju_best_params = {\n",
        "                        'embedding_dim': emb_dim,\n",
        "                        'lstm_units': lstm_unit,\n",
        "                        'batch_size': batch_size,\n",
        "                        'learning_rate': lr\n",
        "                    }\n",
        "\n",
        "print(\"\\nâœ… Best Hyperparameters Found (Review Toko Baju):\")\n",
        "print(baju_best_params)\n",
        "print(f\"Best Validation Accuracy: {baju_best_val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "1fQ76cNBEe5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C. DeteksiSarkasme.json Dataset"
      ],
      "metadata": {
        "id": "C2441vhmGVCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Load dan Preprocessing Data"
      ],
      "metadata": {
        "id": "tFqdfFrcGhYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load data dari JSON\n",
        "sarkas_texts = []\n",
        "sarkas_labels = []\n",
        "\n",
        "with open('/content/DeteksiSarkasme.json', 'r') as f:\n",
        "    for line in f:\n",
        "        item = json.loads(line)\n",
        "        sarkas_texts.append(item['headline'])\n",
        "        sarkas_labels.append(item['is_sarcastic'])\n",
        "\n",
        "# Split train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sarkas_X_train, sarkas_X_test, sarkas_y_train, sarkas_y_test = train_test_split(\n",
        "    sarkas_texts, sarkas_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenisasi & Padding\n",
        "sarkas_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "sarkas_tokenizer.fit_on_texts(sarkas_X_train)\n",
        "\n",
        "sarkas_X_train_seq = sarkas_tokenizer.texts_to_sequences(sarkas_X_train)\n",
        "sarkas_X_test_seq = sarkas_tokenizer.texts_to_sequences(sarkas_X_test)\n",
        "\n",
        "sarkas_maxlen = 100\n",
        "sarkas_X_train_pad = pad_sequences(sarkas_X_train_seq, maxlen=sarkas_maxlen, padding='post', truncating='post')\n",
        "sarkas_X_test_pad = pad_sequences(sarkas_X_test_seq, maxlen=sarkas_maxlen, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "Zy5oaXiUGb2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) Build dan Train Model"
      ],
      "metadata": {
        "id": "k43o1rmXGmo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert labels to numpy array\n",
        "sarkas_y_train = np.array(sarkas_y_train)\n",
        "sarkas_y_test = np.array(sarkas_y_test)\n",
        "\n",
        "sarkas_model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=64, input_length=sarkas_maxlen),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "sarkas_model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "sarkas_history = sarkas_model.fit(\n",
        "    sarkas_X_train_pad, sarkas_y_train,\n",
        "    validation_data=(sarkas_X_test_pad, sarkas_y_test),\n",
        "    epochs=5,\n",
        "    batch_size=128\n",
        ")\n",
        "\n",
        "# Evaluasi awal\n",
        "sarkas_loss, sarkas_acc = sarkas_model.evaluate(sarkas_X_test_pad, sarkas_y_test)\n",
        "print(f\"\\nTest Accuracy (Deteksi Sarkasme): {sarkas_acc:.4f}\")"
      ],
      "metadata": {
        "id": "w8jD1v_xGpTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Matriks Evaluasi dan Visualisasi"
      ],
      "metadata": {
        "id": "MmRODjxcHDM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prediksi probabilitas dan kelas\n",
        "sarkas_y_pred_prob = sarkas_model.predict(sarkas_X_test_pad)\n",
        "sarkas_y_pred = (sarkas_y_pred_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report (Deteksi Sarkasme):\")\n",
        "print(classification_report(sarkas_y_test, sarkas_y_pred, digits=4))\n",
        "\n",
        "# ROC Curve dan AUC\n",
        "sarkas_auc = roc_auc_score(sarkas_y_test, sarkas_y_pred_prob)\n",
        "sarkas_fpr, sarkas_tpr, _ = roc_curve(sarkas_y_test, sarkas_y_pred_prob)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(sarkas_fpr, sarkas_tpr, label=f'ROC Curve (AUC = {sarkas_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Deteksi Sarkasme')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Visualisasi Akurasi dan Loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(sarkas_history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(sarkas_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy - Deteksi Sarkasme')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(sarkas_history.history['loss'], label='Train Loss')\n",
        "plt.plot(sarkas_history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss - Deteksi Sarkasme')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zkfv3NF2HGnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4) Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "xUhPL9r0HMUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "# Pastikan label sudah dalam bentuk numpy array\n",
        "sarkas_y_train = np.array(sarkas_y_train)\n",
        "sarkas_y_test = np.array(sarkas_y_test)\n",
        "\n",
        "# Hyperparameter grids\n",
        "sarkas_embedding_dims = [32, 64]\n",
        "sarkas_lstm_units = [32, 64]\n",
        "sarkas_batch_sizes = [64, 128]\n",
        "sarkas_learning_rates = [1e-3, 5e-4]\n",
        "\n",
        "sarkas_best_val_acc = 0\n",
        "sarkas_best_model = None\n",
        "sarkas_best_params = {}\n",
        "\n",
        "for emb_dim in sarkas_embedding_dims:\n",
        "    for lstm_unit in sarkas_lstm_units:\n",
        "        for batch_size in sarkas_batch_sizes:\n",
        "            for lr in sarkas_learning_rates:\n",
        "                print(f\"\\nðŸ”§ Training with emb_dim={emb_dim}, lstm_units={lstm_unit}, batch_size={batch_size}, lr={lr}\")\n",
        "\n",
        "                model = Sequential([\n",
        "                    Embedding(input_dim=10000, output_dim=emb_dim, input_length=sarkas_maxlen),\n",
        "                    Bidirectional(LSTM(lstm_unit)),\n",
        "                    Dense(64, activation='relu'),\n",
        "                    Dense(1, activation='sigmoid')\n",
        "                ])\n",
        "\n",
        "                model.compile(\n",
        "                    loss='binary_crossentropy',\n",
        "                    optimizer=Adam(learning_rate=lr),\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                history = model.fit(\n",
        "                    sarkas_X_train_pad, sarkas_y_train,\n",
        "                    validation_data=(sarkas_X_test_pad, sarkas_y_test),\n",
        "                    epochs=3,  # cepat untuk tuning\n",
        "                    batch_size=batch_size,\n",
        "                    verbose=0\n",
        "                )\n",
        "\n",
        "                val_acc = history.history['val_accuracy'][-1]\n",
        "                print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "                if val_acc > sarkas_best_val_acc:\n",
        "                    sarkas_best_val_acc = val_acc\n",
        "                    sarkas_best_model = model\n",
        "                    sarkas_best_params = {\n",
        "                        'embedding_dim': emb_dim,\n",
        "                        'lstm_units': lstm_unit,\n",
        "                        'batch_size': batch_size,\n",
        "                        'learning_rate': lr\n",
        "                    }\n",
        "\n",
        "print(\"\\nâœ… Best Hyperparameters Found (Deteksi Sarkasme):\")\n",
        "print(sarkas_best_params)\n",
        "print(f\"Best Validation Accuracy: {sarkas_best_val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "bBBe4Vw0HORS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}